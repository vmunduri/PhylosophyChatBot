{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed9e4d4-110f-4c16-8364-ea7b7c6bce0e",
   "metadata": {},
   "source": [
    "#### Retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d83dc366-40b1-4469-be3e-f9d6968dac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.base import Embeddings\n",
    "import numpy as np\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings): #We need to inherit from Embeddings for integrating into langchain pipeline\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self,texts):\n",
    "        return [self.model.encode(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        return self.model.encode(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae8e806-fca1-41b6-9a47-53f8b657f88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75dd1cfebd234f979a236a125166904d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67effb9e4a884c03aa15e66d91bcd128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2079fb4044cf4168b8be4459a15a28f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd5147cd66a340feb9631e55c15c3c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda4c7916601475d902358b5dff3eb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4958770fd9914f7b91e39b28c50a1dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16391555d1e841d8a1ba57272f91e604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe3c574c1fc49baa832a6efc3efd929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c847205a4ba45af86f208976f208752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4edac3fe39e140b5b8e81f91a17e0988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99d0aa8e1c74c8e9583dff1ce43715b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1d91c9b-9095-4917-88c1-c79e69c03a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embeddings = SentenceTransformerEmbeddings(embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85838881-a527-481c-9fbf-cc7673be8e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db = FAISS.load_local(\"faiss_index\", sentence_embeddings, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16bf8a43-9784-4fa4-8aa1-c7aafcfe0c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_478/2456047785.py:4: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use invoke instead.\n",
      "  retrieved_docs = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "systematics, the field of biology that studies the diversity of life and the relationships of living things through time. Today, systematists typically treat “relatedness” solely in terms of recency of common ancestry, but this was not always the case. Pre-Darwinian taxonomists discussed the relationships of various groups and their place in the “natural system”, and while the rise of evolutionary theory allowed that one sense of relatedness was genealogical, it did not eliminate the idea of the broader notion. Debates about the role of phylogeny in classification and taxonomy were widespread (e.g., Huxley [ed.] 1940; Winsor 1995) though they began to take on a new form beginning in the late 1950s as collaborations turned into organized research programs pushing their agendas.  In his analysis of the period, David Hull (1988: ch. 5) titled one of his chapters “Systematists at War” and thus the name “The Systematics Wars” is sometimes used to describe the debates of the period. Hull (1970) influentially compared three “contemporary systematic philosophies”, typically called pheneticism, (or numerical taxonomy, after the seminal Sokal &amp; Sneath (1963) textbook of the same name), cladistics or phylogenetic systematics (after Hennig’s 1966 foundational text, an earlier version of which was published in German in 1950), and evolutionary systematics (following the lead of proponents like Simpson [1961] and Mayr [1969] and others).  The pheneticists disputed that we were in a\n",
      "\n",
      "\n",
      "and justify their inferences, disagreements over taxonomic classifications faded into the background. On this account, it is disagreements over phylogenetic hypotheses that are impactful; disputes over classifications become less consequential as they get replaced in scientific inference by phylogenies. Joseph Felsenstein (2004: 145) dubs this the “It-Doesn’t-Matter-Very-Much school”, arguing that “systematists ‘voted with their feet’ to establish this school, long before I announced its existence”. How phylogenetics ought to influence, inform, or constrain taxonomic classifications is still a live debate (see Ereshefsky 2001 as well as debates over the adoption of the PhyloCode, among other debates about phylogenetic taxonomy), though one that is orthogonal to the inference of phylogenies. Contemporary debates on taxonomic classification are largely over how to draw inferences from phylogenies for classifications; there is very little dispute over the coherency of reconstructing phylogenies. Biological classification, on this account, is not viewed as a competitor to phylogenetics, but dependent on it—and not the other way around. Phylogeneticists, like Felsenstein, can largely proceed in reconstructing phylogenies without paying attention to classificatory questions, though some of the metaphysical questions about the units of phylogenetics can come back to impact the inference of phylogeny itself (see  §2.6.2)  As an important aside, we say ‘largely’ in the previous\n",
      "\n",
      "\n",
      "are preferable for reconstructing phylogenetic history. From those phylogenies, analogies can be better identified, offering evidence for appeals to natural selection (in the form of convergent evolution or other evolutionary processes) as an explanation for the pattern of biodiversity and trait distribution (Whittall &amp; Hodges 2007 provides a good example of this reasoning in regard to pollinator mode and morphology). Put another way, molecular data helps biologists explain why what appear to be homologies fail to form nested hierarchies. Molecular data can identify which of those might be analogies instead, and this helps resolve what would otherwise appear to be conflicts in phylogenetic inference. This is one way biologists use phylogenies to offer data-driven, theoretically-grounded evolutionary explanations, in contrast to “just-so” stories.  In addition to adding more evidence in cases where systematists already had a great deal of it, molecular studies can shed light on cases where other sources of evidence are extremely weak. For example, molecular data may preserve deep phylogenetic history otherwise obscured at the morphological level by millions of years of adaptive changes. Phylogeneticists depicting the very earliest branches in plant and animal phylogenies, as well as relationships between the different groups of eukaryotes, recognized how molecular data could both test and enrich these deep phylogenetic hypotheses (Kenrick &amp; Crane 1997; Baldauf 2003).\n",
      "\n",
      "\n",
      "Wiley 1981). For many phylogeneticists this goal continues to this day—most recently around the development of the PhyloCode (see  Other Internet Resources;  de Queiroz &amp; Gauthier 1990, 1992, 1994; Cantino &amp; de Queiroz 2020). Though structural reforms to wholly replace Linnaean taxonomy have struggled to gain wide-scale traction and adoption, taxonomists have largely adopted monophyly as a guiding principle in classification and systematics—even in the context of so-called traditional taxonomy (though microbial systematics has proven more resistant; see  §3.1).  Of course, how biologists reconstruct a phylogeny and identify monophyletic groups is a matter of taxonomic freedom on the PhyloCode and other phylogenetic classifications. That’s just to say that phylogenetic inference will operate largely independently of these metaphysical and classificatory issues—except insofar as these classifications inform biologists about what it is that they are aiming to reconstruct a history of in the first place. Haber (2019) provides a framing for this challenge, arguing that the units of phylogeny are the units of divergence and diversification. Where Ereshefsky (2001) and other advocates of phylogenetic classification provide a specific way of cashing out those units, Haber describes this as a rich, general research project, similar to other “unit” problems in philosophy of biology. In much the same way that how we cash out, say, the units of selection may impact how we study\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform a query by using the retriever\n",
    "retriever = db.as_retriever()  # Convert FAISS into a retriever object\n",
    "query = \"What is phylosophy?\"\n",
    "retrieved_docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# Print out the retrieved documents\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.page_content)\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7db87c-2169-4cf2-a430-28a9fee3595b",
   "metadata": {},
   "source": [
    "#### Question Answering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9a7a5be-c812-4f11-aca1-89b654c88297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8012ae4ac363499d8d23009096622330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f009d6278b97402eacd6706cd707e5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/31.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f75331b862f0491291320cb5ff5eca91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f710715479a94d3fa6f7d59da2123a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec02fae0fdc4f479f758babb6ac61f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "812057e62e824e00883bfbcd51888550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af290dcfaae4030ad42a284be77e171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/2.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e09ffe28754ed990e0a76f118122ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c06b9fcf1b14617a3cb586fe29bfbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/212 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/CodeQwen1.5-7B-Chat\",\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8002fbe0-d3ce-4086-ba05-b4090caa65a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d006169da1d40e2b678dbf8761caeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/972 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1a714381634283be980f43b4c5f53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/CodeQwen1.5-7B-Chat\")\n",
    "\n",
    "prompt = \"how to enjoy life\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "345ebf5e-6ea8-4d36-b18e-ed7f361e8055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate([\n",
    "     ('system', \"You are a helpful assistant.\"),\n",
    "    ('user', \"{input}\")\n",
    "])\n",
    "\n",
    "prompt  = template.invoke({'input': 'how to enjoy life?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85b95b52-db98-4431-8883-aa58edd7c3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6872fd5-e4cf-48f5-9032-a5a7d3689baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nEnjoying life can vary from person to person, but here are some tips to help you find what truly makes you happy:\\n\\n1. Identify your passions and interests: This will help you make time for things that you enjoy doing, which can lead to a more fulfilling life overall.\\n\\n2. Set goals: Having clear and specific goals can help you stay motivated and engaged. Setting goals and working towards them can create a sense of accomplishment, which can lead to greater happiness.\\n\\n3. Spend time with loved ones: Making time for people you care about can boost your mood and increase your overall sense of happiness.\\n\\n4. Practice gratitude: Focusing on the good things in your life and expressing gratitude can help you feel more content and fulfilled.\\n\\n5. Practice mindfulness: Mindfulness can help you become more present in your life and enjoy the present moment.\\n\\n6. Give back to others: Making a positive impact on others, whether by volunteering, supporting causes you care about, or being a good neighbor, can increase your sense of happiness and fulfillment.\\n\\n7. Engage in activities that you find relaxing: Activities that you enjoy can help you feel more calm and relaxed, which can in turn lead to a more fulfilling life.\\n\\nRemember, happiness is subjective and what one person finds enjoyable may not be the same for another. However, by identifying your passions and interests, setting goals, spending time with loved ones, practicing gratitude, practicing mindfulness, giving back to others, and engaging in activities that you find relaxing, you can increase your chances of experiencing more happiness.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6049860-5633-433c-b103-e4cc43f14d0c",
   "metadata": {},
   "source": [
    "#### Add retrieval to the question answering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfb923bb-1581-465d-b6e2-7a46125154f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "# chat prompt template is used for running model in conversational format\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"You're name is Dr.Phil\"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "template = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": '{input}'}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1213f877-6a38-41e9-98eb-5d108ad86c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_template(system_prompt, context, user_input): #Custom prompt template\n",
    "    template = [\n",
    "        {'role':'system', 'content': system_prompt.format(context = context)},\n",
    "        {'role': 'user', 'content': user_input}\n",
    "    ]\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f0fca5b-4658-4d92-b5e4-c2aafb0d339e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain_core.runnables import Runnable\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class CustomLLMRunnable(Runnable):\n",
    "    def invoke(self, messages, **kwargs):\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda')\n",
    "        \n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=512\n",
    "        )\n",
    "        \n",
    "        # Slice generated tokens to exclude input\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        \n",
    "        # Decode the response and return it\n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        return response\n",
    "\n",
    "custom_llm_runnable = CustomLLMRunnable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b037b57b-9045-4369-98c6-cefba38edc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = retriever.get_relevant_documents('what is phylosophy?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8b6fcc4-88d6-472f-b6ce-6c25beda585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer_chain(system_prompt, context, question):\n",
    "    prompt = format_template(system_prompt, context, question)\n",
    "    return custom_llm_runnable.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18afc716-044e-4bf6-8bc6-a1bf8b7662ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_chain(question):\n",
    "    context = retriever.get_relevant_documents(question)\n",
    "    context = [document.page_content for document in context]\n",
    "    return question_answer_chain(system_prompt, context, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8caa658f-105d-42a7-ab7a-c1275f5b193c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In many evolutionary theory and psychological literature, it is common to view altruism as a category of behavior characterized by acts that result from self-interested reasons or are undertaken as a form of self-protection. However, it is worth noting that there are instances where acts of kindness are driven by true altruism, which is characterized by an intention to promote the welfare of others without any personal gain.\\n\\nOne example of true altruism is the act of selfless service, in which an individual performs a service to another human being out of a genuine desire to help and improve their well-being. For example, a person might volunteer their time and skills at a local animal shelter to help those in need, even if it involves sacrificing their own interests or well-being.\\n\\nHowever, it's important to understand that true altruism is not always the best motive for a given act. In many cases, acts of kindness may arise from self-protection or self-compassion, which may lead to emotional sacrifice or sacrifice for one's own well-being. Therefore, it is essential to distinguish between pure altruism and self-interested behavior.\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain('Is there such a thing as true altruism, or are all acts of kindness ultimately driven by self-interest?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517f7f9-db7c-48b6-8b4c-91f5d6171d8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
